{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "name": "ImageCaptioningFlickr8.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/duttagoutam/AIML/blob/master/ImageCaptioningFlickr8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTEpDQdf7kQN",
        "colab_type": "text"
      },
      "source": [
        "# Image Captioning Flickr8k Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRNGEmrTHQZN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "815a1c94-4bae-43d6-e675-20564b58e941"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBHosj4B7kQS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import string\n",
        "import sys, time, os, warnings \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pydotplus\n",
        "from pandas import DataFrame\n",
        "from array import array\n",
        "from os import listdir\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import to_categorical\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import plot_model\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Activation\n",
        "from keras.layers.merge import add\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import load_model\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from numpy import argmax\n",
        "from pickle import dump\n",
        "from pickle import load\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "from keras.layers.pooling import GlobalMaxPooling2D\n",
        "from keras.layers import RepeatVector\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.layers import Embedding\n",
        "from keras.layers.merge import concatenate\n",
        "#keras.utils.vis_utils.pydot = pydot\n",
        "#from keras.backend.tensorflow_backend import set_session\n",
        "#from collections import Counter "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sTjBmR3Sr2C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "folderName='/content/drive/My Drive/App/storage/Flickr8k_Dataset'\n",
        "imageDir=folderName+'/Images'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NB5R8JOv7kQf",
        "colab_type": "text"
      },
      "source": [
        "## Load Photographs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQxsFP7S7kQp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Not To Run Always\n",
        "npix = 224\n",
        "target_size = (npix,npix)\n",
        "\n",
        "def load_photos(directory):\n",
        "    images = dict()\n",
        "    for name in listdir(directory):\n",
        "        # load an image from file\n",
        "        #print(\"name\",name)\n",
        "        filename = directory + '/' + name\n",
        "        image = load_img(filename, target_size)\n",
        "        # convert the image pixels to a numpy array\n",
        "        image = img_to_array(image)\n",
        "        # reshape data for the model\n",
        "        image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "        # prepare the image for the VGG model\n",
        "        image = preprocess_input(image)\n",
        "        # get image id\n",
        "        image_id = name.split('.')[0]\n",
        "        images[image_id] = image\n",
        "    return images"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUtyor3l7kQv",
        "colab_type": "code",
        "outputId": "72fba2ea-1e5a-4e51-eb47-03b59258a61a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "# Not To Run Always\n",
        "# load images\n",
        "#directory = 'Flicker8k_Dataset'\n",
        "directory = imageDir\n",
        "images = load_photos(directory)\n",
        "print('Loaded Images: %d' % len(images))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-ba2474b92e5b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdirectory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimageDir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_photos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loaded Images: %d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-9f3c0c0d57ad>\u001b[0m in \u001b[0;36mload_photos\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;31m#print(\"name\",name)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdirectory\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0;31m# convert the image pixels to a numpy array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/utils.py\u001b[0m in \u001b[0;36mload_img\u001b[0;34m(path, grayscale, color_mode, target_size, interpolation)\u001b[0m\n\u001b[1;32m    108\u001b[0m         raise ImportError('Could not import PIL.Image. '\n\u001b[1;32m    109\u001b[0m                           'The use of `load_img` requires PIL.')\n\u001b[0;32m--> 110\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpil_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcolor_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'grayscale'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'L'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2537\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2539\u001b[0;31m     \u001b[0mprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2541\u001b[0m     \u001b[0mpreinit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qotlxBO7kQ1",
        "colab_type": "text"
      },
      "source": [
        "## Extract Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7KJzMmU7kQ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Not To Run Always\n",
        "# extract features from each photo in the directory\n",
        "def extract_features(directory):    \n",
        "    model = VGG16()\n",
        "    model.layers.pop()\n",
        "    model = Model(inputs=model.inputs, outputs=model.layers[-1].output)\n",
        "    print(model.summary())\n",
        "    features = dict()\n",
        "    for name in listdir(directory):\n",
        "        filename = directory + '/' + name\n",
        "        image = load_img(filename, target_size=(224, 224))\n",
        "        image = img_to_array(image)\n",
        "        image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "        image = preprocess_input(image)\n",
        "        feature = model.predict(image, verbose=0)\n",
        "        image_id = name.split('.')[0]\n",
        "        features[image_id] = feature\n",
        "        #print('>%s' % name)\n",
        "    return features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1lg18ha7kQ5",
        "colab_type": "code",
        "outputId": "4b8c67b0-5b92-4021-bc1b-2af21f7cd4e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Not To Run Always\n",
        "# extract features from all images\n",
        "#directory = 'Flickr8k_Dataset'\n",
        "directory = imageDir\n",
        "features = extract_features(directory)\n",
        "print('Extracted Features: %d' % len(features))\n",
        "# save to file\n",
        "dump(features, open(folderName+'/features.pkl', 'wb'))\n",
        "print('Image Features: %d' % len(features))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n",
            "553467904/553467096 [==============================] - 6s 0us/step\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 25088)             0         \n",
            "_________________________________________________________________\n",
            "fc1 (Dense)                  (None, 4096)              102764544 \n",
            "_________________________________________________________________\n",
            "fc2 (Dense)                  (None, 4096)              16781312  \n",
            "=================================================================\n",
            "Total params: 134,260,544\n",
            "Trainable params: 134,260,544\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-8743ff0cc795>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdirectory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimageDir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Extracted Features: %d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# save to file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolderName\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/features.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-67b9f1a214d2>\u001b[0m in \u001b[0;36mextract_features\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mfeature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mimage_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimage_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1399\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1400\u001b[0m                                             \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1401\u001b[0;31m                                             callbacks=callbacks)\n\u001b[0m\u001b[1;32m   1402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1403\u001b[0m     def train_on_batch(self, x, y,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0mbatch_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'batch'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'size'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'predict'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'begin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2977\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2979\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2980\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2981\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2935\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2937\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2938\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCXu6ptA7kQ8",
        "colab_type": "text"
      },
      "source": [
        "## Prepare Text Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fek1Lhbm7kQ9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "    # open the file as read only\n",
        "    file = open(filename, 'r')\n",
        "    # read all text\n",
        "    text = file.read()\n",
        "    # close the file\n",
        "    file.close()\n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SOsE6SaRvWS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Not To Run Always\n",
        "# extract descriptions for images\n",
        "def load_descriptions(doc):\n",
        "    mapping = dict()\n",
        "    # process lines\n",
        "    for line in doc.split('\\n'):\n",
        "        # split line by white space\n",
        "        tokens = line.split()\n",
        "        if len(line) < 2:\n",
        "            continue\n",
        "        # take the first token as the image id, the rest as the description\n",
        "        image_id, image_desc = tokens[0], tokens[1:]\n",
        "        # remove filename from image id\n",
        "        image_id = image_id.split('.')[0]\n",
        "        # convert description tokens back to string\n",
        "        image_desc = ' '.join(image_desc)\n",
        "        # create the list if needed\n",
        "        if image_id not in mapping:\n",
        "            mapping[image_id] = list()\n",
        "        # store description\n",
        "        mapping[image_id].append(image_desc)\n",
        "    return mapping"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msQo-8qLR4Gb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Not To Run Always\n",
        "def clean_descriptions(descriptions):\n",
        "    # prepare translation table for removing punctuation\n",
        "    table = str.maketrans('', '', string.punctuation)\n",
        "    for key, desc_list in descriptions.items():\n",
        "        for i in range(len(desc_list)):\n",
        "            desc = desc_list[i]\n",
        "            # tokenize\n",
        "            desc = desc.split()\n",
        "            # convert to lower case\n",
        "            desc = [word.lower() for word in desc]\n",
        "            # remove punctuation from each token\n",
        "            desc = [w.translate(table) for w in desc]\n",
        "            # remove hanging 's' and 'a'\n",
        "            desc = [word for word in desc if len(word)>1]\n",
        "            # remove tokens with numbers in them\n",
        "            desc = [word for word in desc if word.isalpha()]\n",
        "            # store as string\n",
        "            desc_list[i] =  ' '.join(desc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7TdjNTnR7bM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Not To Run Always\n",
        "# convert the loaded descriptions into a vocabulary of words\n",
        "def to_vocabulary(descriptions):\n",
        "    # build a list of all description strings\n",
        "    all_desc = set()\n",
        "    for key in descriptions.keys():\n",
        "        [all_desc.update(d.split()) for d in descriptions[key]]\n",
        "    return all_desc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34a5OvGMR_dJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Not To Run Always\n",
        "# save descriptions to file, one per line\n",
        "def save_descriptions(descriptions, filename):\n",
        "    lines = list()\n",
        "    for key, desc_list in descriptions.items():\n",
        "        for desc in desc_list:\n",
        "            lines.append(key + ' ' + desc)\n",
        "    data = '\\n'.join(lines)\n",
        "    file = open(filename, 'w')\n",
        "    file.write(data)\n",
        "    file.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0K_hroMSDQe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "d1c5cd45-0d7c-4464-da7d-117793b1356c"
      },
      "source": [
        "# Not To Run Always\n",
        "filename = folderName+'/Flickr_TextData/Flickr8k.token.txt'\n",
        "# load descriptions\n",
        "doc = load_doc(filename)\n",
        "# parse descriptions\n",
        "descriptions = load_descriptions(doc)\n",
        "print('Loaded: %d ' % len(descriptions))\n",
        "# clean descriptions\n",
        "clean_descriptions(descriptions)\n",
        "# summarize vocabulary\n",
        "vocabulary = to_vocabulary(descriptions)\n",
        "print('Vocabulary Size: %d' % len(vocabulary))\n",
        "# save to file\n",
        "save_descriptions(descriptions, folderName+'/descriptions.txt')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded: 8092 \n",
            "Vocabulary Size: 8763\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjGtlLBaSbOf",
        "colab_type": "text"
      },
      "source": [
        "# Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWy27K4q7kQ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load a pre-defined list of photo identifiers\n",
        "def load_set(filename):\n",
        "    doc = load_doc(filename)\n",
        "    dataset = list()\n",
        "    # process line by line\n",
        "    for line in doc.split('\\n'):\n",
        "        # skip empty lines\n",
        "        if len(line) < 1:\n",
        "            continue\n",
        "        # get the image identifier\n",
        "        identifier = line.split('.')[0]\n",
        "        dataset.append(identifier)\n",
        "    return set(dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XWjFQdx7kRC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load clean descriptions into memory\n",
        "def load_clean_descriptions(filename, dataset):\n",
        "    # load document\n",
        "    doc = load_doc(filename)\n",
        "    descriptions = dict()\n",
        "    for line in doc.split('\\n'):\n",
        "        # split line by white space\n",
        "        tokens = line.split()\n",
        "        # split id from description\n",
        "        image_id, image_desc = tokens[0], tokens[1:]\n",
        "        # skip images not in the set\n",
        "        if image_id in dataset:\n",
        "            # create list\n",
        "            if image_id not in descriptions:\n",
        "                descriptions[image_id] = list()\n",
        "            # wrap description in tokens\n",
        "            desc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n",
        "            # store\n",
        "            descriptions[image_id].append(desc)\n",
        "    return descriptions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "if9FX7J17kRG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load photo features\n",
        "def load_photo_features(filename, dataset):\n",
        "    # load all features\n",
        "    all_features = load(open(filename, 'rb'))\n",
        "    # filter features\n",
        "    features = {k: all_features[k] for k in dataset}\n",
        "    return features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t29K8tC97kRK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# covert a dictionary of clean descriptions to a list of descriptions\n",
        "def to_lines(descriptions):\n",
        "    all_desc = list()\n",
        "    for key in descriptions.keys():\n",
        "        [all_desc.append(d) for d in descriptions[key]]\n",
        "    return all_desc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulBuEKtq7kRN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# fit a tokenizer given caption descriptions\n",
        "def create_tokenizer(descriptions):\n",
        "    lines = to_lines(descriptions)\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(lines)\n",
        "    return tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kO2IhtK_7kRQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# calculate the length of the description with the most words\n",
        "def maximum_length(descriptions):\n",
        "    lines = to_lines(descriptions)\n",
        "    return max(len(d.split()) for d in lines)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_-bg7tF7kRT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Not Working\n",
        "# create sequences of images, input sequences and output words for an image\n",
        "def create_sequences(tokenizer, max_length, descriptions, photos, vocab_size):\n",
        "    X1, X2, y = list(), list(), list()\n",
        "    #X1, X2, y = [], [], []\n",
        "    # walk through each image identifier\n",
        "    for key, desc_list in descriptions.items():\n",
        "        # walk through each description for the image\n",
        "        for desc in desc_list:\n",
        "            # encode the sequence\n",
        "            seq = tokenizer.texts_to_sequences([desc])[0]\n",
        "            # split one sequence into multiple X,y pairs\n",
        "            for i in range(1, len(seq)):\n",
        "                # split into input and output pair\n",
        "                in_seq, out_seq = seq[:i], seq[i]\n",
        "                # pad input sequence\n",
        "                in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
        "                # encode output sequence\n",
        "                out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
        "                # store\n",
        "                X1.append(photos[key][0])\n",
        "                X2.append(in_seq)\n",
        "                y.append(out_seq)\n",
        "        #X1 = np.array(X1)\n",
        "        #X2 = np.array(X2)\n",
        "        #y  = np.array(y)\n",
        "    \n",
        "    return np.array(X1), np.array(X2), np.array(y)\n",
        "    #return (X1, X2, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3y2CaP-mydM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create sequences of images, input sequences and output words for an image\n",
        "def create_sequences(tokenizer, max_length, desc_list, photo, vocab_size):\n",
        "\tX1, X2, y = list(), list(), list()\n",
        "\t# walk through each description for the image\n",
        "\tfor desc in desc_list:\n",
        "\t\t# encode the sequence\n",
        "\t\tseq = tokenizer.texts_to_sequences([desc])[0]\n",
        "\t\t# split one sequence into multiple X,y pairs\n",
        "\t\tfor i in range(1, len(seq)):\n",
        "\t\t\t# split into input and output pair\n",
        "\t\t\tin_seq, out_seq = seq[:i], seq[i]\n",
        "\t\t\t# pad input sequence\n",
        "\t\t\tin_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
        "\t\t\t# encode output sequence\n",
        "\t\t\tout_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
        "\t\t\t# store\n",
        "\t\t\tX1.append(photo)\n",
        "\t\t\tX2.append(in_seq)\n",
        "\t\t\ty.append(out_seq)\n",
        "\treturn np.array(X1), np.array(X2), np.array(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftJ-axM-7kRW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define the captioning model\n",
        "def define_model(vocab_size, max_length):\n",
        "    # feature extractor model\n",
        "    inputs1 = Input(shape=(4096,))\n",
        "    fe1 = Dropout(0.5)(inputs1)\n",
        "    fe2 = Dense(256, activation='relu')(fe1)\n",
        "    # sequence model\n",
        "    inputs2 = Input(shape=(max_length,))\n",
        "    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
        "    se2 = Dropout(0.5)(se1)\n",
        "    se3 = LSTM(256)(se2)\n",
        "    # decoder model\n",
        "    decoder1 = add([fe2, se3])\n",
        "    decoder2 = Dense(256, activation='relu')(decoder1)\n",
        "    outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
        "    # tie it together [image, seq] [word]\n",
        "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "    # summarize model\n",
        "    print(model.summary())\n",
        "    #plot_model(model, to_file='model.png', show_shapes=True)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4Iv0XOJShHj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# data generator, intended to be used in a call to model.fit_generator()\n",
        "def data_generator(descriptions, photos, tokenizer, max_length, vocab_size):\n",
        "\t# loop for ever over images\n",
        "\twhile 1:\n",
        "\t\tfor key, desc_list in descriptions.items():\n",
        "\t\t\t# retrieve the photo feature\n",
        "\t\t\tphoto = photos[key][0]\n",
        "\t\t\tin_img, in_seq, out_word = create_sequences(tokenizer, max_length, desc_list, photo, vocab_size)\n",
        "\t\t\tyield [[in_img, in_seq], out_word]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Li9o9VqR7kRY",
        "colab_type": "text"
      },
      "source": [
        "## Train Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2b1YCDr7kRZ",
        "colab_type": "code",
        "outputId": "cc315db2-456c-4014-a77a-8863f6be993e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "# Not Working\n",
        "# load training dataset (6K)\n",
        "#filename = 'Flickr8k_text/Flickr_8k.trainImages.txt'\n",
        "filename = folderName+'/Flickr_TextData/Flickr_8k.trainImages.txt'\n",
        "train = load_set(filename)\n",
        "print('Dataset: %d' % len(train))\n",
        "# descriptions\n",
        "train_descriptions = load_clean_descriptions(folderName+'/descriptions.txt', train)\n",
        "print('Descriptions: train=%d' % len(train_descriptions))\n",
        "# photo features\n",
        "train_features = load_photo_features(folderName+'/features.pkl', train)\n",
        "print('Photos: train=%d' % len(train_features))\n",
        "# prepare tokenizer\n",
        "tokenizer = create_tokenizer(train_descriptions)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print('Vocabulary Size: %d' % vocab_size)\n",
        "# determine the maximum sequence length\n",
        "max_length = maximum_length(train_descriptions)\n",
        "print('Description Length: %d' % max_length)\n",
        "# prepare sequences\n",
        "X1train, X2train, ytrain = create_sequences(tokenizer, max_length, train_descriptions, train_features, vocab_size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset: 6000\n",
            "Descriptions: train=6000\n",
            "Photos: train=6000\n",
            "Vocabulary Size: 7579\n",
            "Description Length: 34\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-2bRqC17kRc",
        "colab_type": "text"
      },
      "source": [
        "## Test dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBcsCu8g7kRc",
        "colab_type": "code",
        "outputId": "acc48293-0d6e-4c23-8f85-7c5717c7b746",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        }
      },
      "source": [
        "# Not Working\n",
        "# load test set\n",
        "#filename = 'Flickr8k_text/Flickr_8k.devImages.txt'\n",
        "filename = folderName+'/Flickr_TextData/Flickr_8k.devImages.txt'\n",
        "test = load_set(filename)\n",
        "print('Dataset: %d' % len(test))\n",
        "# descriptions\n",
        "test_descriptions = load_clean_descriptions(folderName+'/descriptions.txt', test)\n",
        "print('Descriptions: test=%d' % len(test_descriptions))\n",
        "# photo features\n",
        "test_features = load_photo_features(folderName+'/features.pkl', test)\n",
        "print('Photos: test=%d' % len(test_features))\n",
        "# prepare sequences\n",
        "X1test, X2test, ytest = create_sequences(tokenizer, max_length, test_descriptions, test_features, vocab_size)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-911d68e8c937>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfolderName\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/Flickr_TextData/Flickr_8k.devImages.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Dataset: %d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# descriptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtest_descriptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_clean_descriptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolderName\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/descriptions.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'folderName' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXTwMVY87kRf",
        "colab_type": "text"
      },
      "source": [
        "## Fit The Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-DTJsa07kRg",
        "colab_type": "code",
        "outputId": "55237f2e-5592-4ab8-d91d-00c03542f658",
        "colab": {}
      },
      "source": [
        "# Not Working\n",
        "# define the model\n",
        "model = define_model(vocab_size, max_length)\n",
        "# define checkpoint callback\n",
        "filepath = folderName+'/'+'model-ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5'\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "# fit model\n",
        "model.fit([X1train, X2train], ytrain, epochs=10, verbose=2, callbacks=[checkpoint], validation_data=([X1test, X2test], ytest))\n",
        "print('Model Fitting Done Successfully')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            (None, 34)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_1 (InputLayer)            (None, 4096)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, 34, 256)      1940224     input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 4096)         0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 34, 256)      0           embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 256)          1048832     dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 256)          525312      dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 256)          0           dense_1[0][0]                    \n",
            "                                                                 lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 256)          65792       add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 7579)         1947803     dense_2[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 5,527,963\n",
            "Trainable params: 5,527,963\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Train on 306404 samples, validate on 50903 samples\n",
            "Epoch 1/20\n",
            " - 668s - loss: 4.5363 - val_loss: 4.0905\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 4.09045, saving model to /storage/Flickr_Data/model-ep001-loss4.536-val_loss4.090.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBN1XJyAVHZz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 850
        },
        "outputId": "8cab16d1-dc52-4112-d1cb-5b9997808aa5"
      },
      "source": [
        "# load training dataset (6K)\n",
        "filename = folderName+'/Flickr_TextData/Flickr_8k.trainImages.txt'\n",
        "train = load_set(filename)\n",
        "print('Dataset: %d' % len(train))\n",
        "# descriptions\n",
        "train_descriptions = load_clean_descriptions(folderName+'/descriptions.txt', train)\n",
        "print('Descriptions: train=%d' % len(train_descriptions))\n",
        "# photo features\n",
        "train_features = load_photo_features(folderName+'/features.pkl', train)\n",
        "print('Photos: train=%d' % len(train_features))\n",
        "# prepare tokenizer\n",
        "tokenizer = create_tokenizer(train_descriptions)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print('Vocabulary Size: %d' % vocab_size)\n",
        "# determine the maximum sequence length\n",
        "max_length = maximum_length(train_descriptions)\n",
        "print('Description Length: %d' % max_length)\n",
        " \n",
        "# define the model\n",
        "model = define_model(vocab_size, max_length)\n",
        "# train the model, run epochs manually and save after each epoch\n",
        "epochs = 10\n",
        "steps = len(train_descriptions)\n",
        "for i in range(epochs):\n",
        "\t# create the data generator\n",
        "\tgenerator = data_generator(train_descriptions, train_features, tokenizer, max_length, vocab_size)\n",
        "\t# fit for one epoch\n",
        "\tmodel.fit_generator(generator, epochs=i, steps_per_epoch=steps, verbose=1)\n",
        "\t# save model\n",
        "  #filepath = folderName+'/'+'model-ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5'\n",
        "\tmodel.save(folderName+'/'+'model_' + str(i) + '.h5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset: 6000\n",
            "Descriptions: train=6000\n",
            "Photos: train=6000\n",
            "Vocabulary Size: 7579\n",
            "Description Length: 34\n",
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_4 (InputLayer)            (None, 34)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_3 (InputLayer)            (None, 4096)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_2 (Embedding)         (None, 34, 256)      1940224     input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 4096)         0           input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 34, 256)      0           embedding_2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 256)          1048832     dropout_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lstm_2 (LSTM)                   (None, 256)          525312      dropout_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 256)          0           dense_4[0][0]                    \n",
            "                                                                 lstm_2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 256)          65792       add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 7579)         1947803     dense_5[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 5,527,963\n",
            "Trainable params: 5,527,963\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/1\n",
            "6000/6000 [==============================] - 2377s 396ms/step - loss: 4.6603\n",
            "Epoch 1/1\n",
            "6000/6000 [==============================] - 2374s 396ms/step - loss: 3.9018\n",
            "Epoch 1/1\n",
            "6000/6000 [==============================] - 2356s 393ms/step - loss: 3.6479\n",
            "Epoch 1/1\n",
            "6000/6000 [==============================] - 2258s 376ms/step - loss: 3.5098\n",
            "Epoch 1/1\n",
            "6000/6000 [==============================] - 2248s 375ms/step - loss: 3.4147\n",
            "Epoch 1/1\n",
            "6000/6000 [==============================] - 2282s 380ms/step - loss: 3.3515\n",
            "Epoch 1/1\n",
            "4783/6000 [======================>.......] - ETA: 7:42 - loss: 3.3046"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DedpORz27kRl",
        "colab_type": "text"
      },
      "source": [
        "## Evaluate Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbewRg4s7kRm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# map an integer to a word\n",
        "def word_for_id(integer, tokenizer):\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == integer:\n",
        "            return word\n",
        "    return None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21zDZBET7kRr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# generate a description for an image\n",
        "def generate_desc(model, tokenizer, photo, max_length):\n",
        "    # seed the generation process\n",
        "    in_text = 'startseq'\n",
        "    # iterate over the whole length of the sequence\n",
        "    for i in range(max_length):\n",
        "        # integer encode input sequence\n",
        "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
        "        # pad input\n",
        "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
        "        # predict next word\n",
        "        yhat = model.predict([photo,sequence], verbose=0)\n",
        "        # convert probability to integer\n",
        "        yhat = argmax(yhat)\n",
        "        # map integer to word\n",
        "        word = word_for_id(yhat, tokenizer)\n",
        "        # stop if we cannot map the word\n",
        "        if word is None:\n",
        "            break\n",
        "        # append as input for generating the next word\n",
        "        in_text += ' ' + word\n",
        "        # stop if we predict the end of the sequence\n",
        "        if word == 'endseq':\n",
        "            break\n",
        "    return in_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2S5RbYb7kRu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_model(model, descriptions, photos, tokenizer, max_length):\n",
        "    actual, predicted = list(), list()\n",
        "    # step over the whole set\n",
        "    for key, desc_list in descriptions.items():\n",
        "        # generate description\n",
        "        yhat = generate_desc(model, tokenizer, photos[key], max_length)\n",
        "        # store actual and predicted\n",
        "        references = [d.split() for d in desc_list]\n",
        "        actual.append(references)\n",
        "        predicted.append(yhat.split())\n",
        "    # calculate BLEU score\n",
        "    print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
        "    print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
        "    print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
        "    print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBJbMi8QUnpy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "ce3726ab-472f-4a4a-8f34-e29ac4681bae"
      },
      "source": [
        "# prepare tokenizer on train set\n",
        " \n",
        "# load training dataset (6K)\n",
        "filename = folderName+'/Flickr_TextData/Flickr_8k.trainImages.txt'\n",
        "train = load_set(filename)\n",
        "print('Dataset: %d' % len(train))\n",
        "# descriptions\n",
        "train_descriptions = load_clean_descriptions(folderName+'/descriptions.txt', train)\n",
        "print('Descriptions: train=%d' % len(train_descriptions))\n",
        "# prepare tokenizer\n",
        "tokenizer = create_tokenizer(train_descriptions)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print('Vocabulary Size: %d' % vocab_size)\n",
        "# determine the maximum sequence length\n",
        "max_length = maximum_length(train_descriptions)\n",
        "print('Description Length: %d' % max_length)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset: 6000\n",
            "Descriptions: train=6000\n",
            "Vocabulary Size: 7579\n",
            "Description Length: 34\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3uwO0jCU0Cy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "cc2c03e6-e015-49ab-954e-deac36298730"
      },
      "source": [
        "# prepare test set\n",
        " \n",
        "# load test set\n",
        "filename = folderName+'/Flickr_TextData/Flickr_8k.testImages.txt'\n",
        "test = load_set(filename)\n",
        "print('Dataset: %d' % len(test))\n",
        "# descriptions\n",
        "test_descriptions = load_clean_descriptions(folderName+'/descriptions.txt', test)\n",
        "print('Descriptions: test=%d' % len(test_descriptions))\n",
        "# photo features\n",
        "test_features = load_photo_features(folderName+'/features.pkl', test)\n",
        "print('Photos: test=%d' % len(test_features))"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset: 1000\n",
            "Descriptions: test=1000\n",
            "Photos: test=1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLbQChM87kRy",
        "colab_type": "code",
        "outputId": "5dd00bcf-8163-4cf9-c6fc-084882a98f6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# load the model\n",
        "#filename = folderName+'/'+'model-ep001-loss4.523-val_loss4.065.h5'\n",
        "filename = folderName+'/model_7.h5'\n",
        "model = load_model(filename)\n",
        "# evaluate model\n",
        "evaluate_model(model, test_descriptions, test_features, tokenizer, max_length)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BLEU-1: 0.527713\n",
            "BLEU-2: 0.277879\n",
            "BLEU-3: 0.186908\n",
            "BLEU-4: 0.085236\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qC_3y_lLVBKY",
        "colab_type": "text"
      },
      "source": [
        "# Generate New Captions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irVleiMZU8gC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "dc27bdbb-80a7-45cc-d7b3-80bb852e5121"
      },
      "source": [
        "# Not run Always\n",
        "# load training dataset (6K)\n",
        "filename = folderName+'/Flickr_TextData/Flickr_8k.trainImages.txt'\n",
        "train = load_set(filename)\n",
        "print('Dataset: %d' % len(train))\n",
        "# descriptions\n",
        "train_descriptions = load_clean_descriptions(folderName+'/descriptions.txt', train)\n",
        "print('Descriptions: train=%d' % len(train_descriptions))\n",
        "# prepare tokenizer\n",
        "tokenizer = create_tokenizer(train_descriptions)\n",
        "# save the tokenizer\n",
        "dump(tokenizer, open(folderName+'/tokenizer.pkl', 'wb'))"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset: 6000\n",
            "Descriptions: train=6000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQ65ld_VWdTU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# extract features from each photo in the directory\n",
        "def extract_features(filename):\n",
        "\t# load the model\n",
        "\tmodel = VGG16()\n",
        "\t# re-structure the model\n",
        "\tmodel.layers.pop()\n",
        "\tmodel = Model(inputs=model.inputs, outputs=model.layers[-1].output)\n",
        "\t# load the photo\n",
        "\timage = load_img(filename, target_size=(224, 224))\n",
        "\t# convert the image pixels to a numpy array\n",
        "\timage = img_to_array(image)\n",
        "\t# reshape data for the model\n",
        "\timage = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "\t# prepare the image for the VGG model\n",
        "\timage = preprocess_input(image)\n",
        "\t# get features\n",
        "\tfeature = model.predict(image, verbose=0)\n",
        "\treturn feature"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iuxy0B507kR1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4c7f0a8e-8110-42d7-d0e7-aaaddf9f5ee1"
      },
      "source": [
        "# load the tokenizer\n",
        "tokenizer = load(open(folderName+'/tokenizer.pkl', 'rb'))\n",
        "# pre-define the max sequence length (from training)\n",
        "max_length = 34\n",
        "# load the model\n",
        "filename = folderName+'/model_7.h5'\n",
        "model = load_model(filename)\n",
        "# load and prepare the photograph\n",
        "imageFileName = imageDir+'/10815824_2997e03d76.jpg'\n",
        "photo = extract_features(imageFileName)\n",
        "# generate description\n",
        "description = generate_desc(model, tokenizer, photo, max_length)\n",
        "print(description)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "startseq man in black shirt is walking on the street endseq\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_cICaRzF5O5",
        "colab_type": "text"
      },
      "source": [
        "#  Fit Dev Images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63st6lv_VK_o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# split a dataset into train/test elements\n",
        "def train_test_split(dataset):\n",
        "\t# order keys so the split is consistent\n",
        "\tordered = sorted(dataset)\n",
        "\t# return split dataset as two new sets\n",
        "\treturn set(ordered[:100]), set(ordered[100:200])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTwyCINBF2uQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "adc5a20b-d787-418a-f616-e41e744b5a27"
      },
      "source": [
        "# load dev set\n",
        "filename = folderName+'/Flickr_TextData/Flickr_8k.devImages.txt'\n",
        "dataset = load_set(filename)\n",
        "print('Dataset: %d' % len(dataset))\n",
        "# train-test split\n",
        "train, test = train_test_split(dataset)\n",
        "print('Train=%d, Test=%d' % (len(train), len(test)))\n",
        "# descriptions\n",
        "train_descriptions = load_clean_descriptions(folderName+'/descriptions.txt', train)\n",
        "test_descriptions = load_clean_descriptions(folderName+'/descriptions.txt', test)\n",
        "print('Descriptions: train=%d, test=%d' % (len(train_descriptions), len(test_descriptions)))\n",
        "# photo features\n",
        "train_features = load_photo_features(folderName+'/features.pkl', train)\n",
        "test_features = load_photo_features(folderName+'/features.pkl', test)\n",
        "print('Photos: train=%d, test=%d' % (len(train_features), len(test_features)))\n",
        "# prepare tokenizer\n",
        "tokenizer = create_tokenizer(train_descriptions)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print('Vocabulary Size: %d' % vocab_size)\n",
        "# determine the maximum sequence length\n",
        "max_length = maximum_length(train_descriptions)\n",
        "print('Description Length: %d' % max_length)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset: 1000\n",
            "Train=100, Test=100\n",
            "Descriptions: train=100, test=100\n",
            "Photos: train=100, test=100\n",
            "Vocabulary Size: 876\n",
            "Description Length: 31\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qp_CY2RDbpF8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# evaluate the skill of the model\n",
        "def evaluate_model_new(model, descriptions, photos, tokenizer, max_length):\n",
        "\tactual, predicted = list(), list()\n",
        "\t# step over the whole set\n",
        "\tfor key, desc in descriptions.items():\n",
        "\t\t# generate description\n",
        "\t\tyhat = generate_desc(model, tokenizer, photos[key], max_length)\n",
        "\t\t# store actual and predicted\n",
        "\t\tactual.append([d.split() for d in desc])\n",
        "\t\tpredicted.append(yhat.split())\n",
        "\tprint('Actual:    %s' % desc)\n",
        "\tprint('Predicted: %s' % yhat)\n",
        "\t# calculate BLEU score\n",
        "\tbleu = corpus_bleu(actual, predicted)\n",
        "\treturn bleu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXFFPEqXGxrn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define experiment\n",
        "def run_experiment(model_name, func_name, verbose, n_epochs, n_photos_per_update, n_batches_per_epoch, n_repeats):\n",
        "  # run experiment\n",
        "  train_results, test_results = list(), list()\n",
        "  for i in range(n_repeats):\n",
        "    # define the model\n",
        "    model = func_name(vocab_size, max_length)\n",
        "    # fit model\n",
        "    #data_generator(train_descriptions, train_features, tokenizer, max_length, n_photos_per_update\n",
        "    model.fit_generator(data_generator(train_descriptions, train_features, tokenizer, max_length, vocab_size), steps_per_epoch=n_batches_per_epoch, epochs=n_epochs, verbose=verbose)\n",
        "    # evaluate model on training data\n",
        "    train_score = evaluate_model_new(model, train_descriptions, train_features, tokenizer, max_length)\n",
        "    test_score = evaluate_model_new(model, test_descriptions, test_features, tokenizer, max_length)\n",
        "    print(\"train_score:\", train_score)\n",
        "    print(\"test_score:\", test_score)\n",
        "    # store\n",
        "    train_results.append(train_score)\n",
        "    test_results.append(test_score)\n",
        "    #print('>%d: train=%f test=%f' % ((i+1), train_score, test_score))\n",
        "  # save results to file\n",
        "  resultFolder = folderName+'/Result'\n",
        "  df = DataFrame()\n",
        "  df['train'] = train_results\n",
        "  df['test'] = test_results\n",
        "  print(df.describe())\n",
        "  df.to_csv(resultFolder+'/'+model_name+'.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtXFuWdyDRgk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c3061155-6b09-4494-9d26-c55d72831bcc"
      },
      "source": [
        "# define experiment\n",
        "model_name = 'baseline1'\n",
        "verbose = 1\n",
        "n_epochs = 10\n",
        "n_photos_per_update = 2\n",
        "n_batches_per_epoch = int(len(train) / n_photos_per_update)\n",
        "n_repeats = 1\n",
        "#func = define_model(vocab_size, max_length)\n",
        "run_experiment(model_name, define_model, verbose, n_epochs, n_photos_per_update, n_batches_per_epoch, n_repeats)"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_12\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_21 (InputLayer)           (None, 31)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_20 (InputLayer)           (None, 4096)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_9 (Embedding)         (None, 31, 256)      224256      input_21[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_17 (Dropout)            (None, 4096)         0           input_20[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_18 (Dropout)            (None, 31, 256)      0           embedding_9[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_25 (Dense)                (None, 256)          1048832     dropout_17[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lstm_9 (LSTM)                   (None, 256)          525312      dropout_18[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_9 (Add)                     (None, 256)          0           dense_25[0][0]                   \n",
            "                                                                 lstm_9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_26 (Dense)                (None, 256)          65792       add_9[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_27 (Dense)                (None, 876)          225132      dense_26[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 2,089,324\n",
            "Trainable params: 2,089,324\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            "50/50 [==============================] - 21s 418ms/step - loss: 6.0929\n",
            "Epoch 2/10\n",
            "50/50 [==============================] - 17s 334ms/step - loss: 5.6973\n",
            "Epoch 3/10\n",
            "50/50 [==============================] - 17s 344ms/step - loss: 5.2234\n",
            "Epoch 4/10\n",
            "50/50 [==============================] - 17s 339ms/step - loss: 5.2784\n",
            "Epoch 5/10\n",
            "50/50 [==============================] - 17s 349ms/step - loss: 4.8344\n",
            "Epoch 6/10\n",
            "50/50 [==============================] - 17s 343ms/step - loss: 4.9245\n",
            "Epoch 7/10\n",
            "50/50 [==============================] - 18s 351ms/step - loss: 4.5928\n",
            "Epoch 8/10\n",
            "50/50 [==============================] - 17s 338ms/step - loss: 4.7913\n",
            "Epoch 9/10\n",
            "50/50 [==============================] - 18s 350ms/step - loss: 4.2501\n",
            "Epoch 10/10\n",
            "50/50 [==============================] - 17s 342ms/step - loss: 4.4257\n",
            "Actual:    ['startseq baby holds game control above its head and laughs endseq', 'startseq baby is holding controller to video game above his head endseq', 'startseq little kid holds up remote control in front of the television endseq', 'startseq young boy is holding playstation controller over his head endseq', 'startseq baby sitting on floor and holding game controller over its head endseq']\n",
            "Predicted: startseq baby holds holds holding holding holding holding endseq\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Actual:    ['startseq man in an orange construction uniform wears white hardhat and sprays water hose endseq', 'startseq man in an orange safety suit spraying water endseq', 'startseq man in public road workers uniform sprays water endseq', 'startseq man wearing an orange uniform is working outside with hose endseq', 'startseq the man in the work uniform is using water hose endseq']\n",
            "Predicted: startseq girl girl people people endseq\n",
            "train_score: 0.12188045378562154\n",
            "test_score: 0.09634775668507153\n",
            "         train      test\n",
            "count  1.00000  1.000000\n",
            "mean   0.12188  0.096348\n",
            "std        NaN       NaN\n",
            "min    0.12188  0.096348\n",
            "25%    0.12188  0.096348\n",
            "50%    0.12188  0.096348\n",
            "75%    0.12188  0.096348\n",
            "max    0.12188  0.096348\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-BklPrtMUauk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1796f762-554a-4396-f885-0ec594f16bb0"
      },
      "source": [
        "# Generate Baseline model\n",
        "model_name = 'baseline_generate'\n",
        "n_repeats = 1\n",
        "run_experiment(model_name, define_model, verbose, n_epochs, n_photos_per_update, n_batches_per_epoch, n_repeats)"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_13\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_23 (InputLayer)           (None, 31)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_22 (InputLayer)           (None, 4096)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_10 (Embedding)        (None, 31, 256)      224256      input_23[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_19 (Dropout)            (None, 4096)         0           input_22[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_20 (Dropout)            (None, 31, 256)      0           embedding_10[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dense_28 (Dense)                (None, 256)          1048832     dropout_19[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lstm_10 (LSTM)                  (None, 256)          525312      dropout_20[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_10 (Add)                    (None, 256)          0           dense_28[0][0]                   \n",
            "                                                                 lstm_10[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_29 (Dense)                (None, 256)          65792       add_10[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_30 (Dense)                (None, 876)          225132      dense_29[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 2,089,324\n",
            "Trainable params: 2,089,324\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            "50/50 [==============================] - 21s 422ms/step - loss: 6.0696\n",
            "Epoch 2/10\n",
            "50/50 [==============================] - 17s 332ms/step - loss: 5.7756\n",
            "Epoch 3/10\n",
            "50/50 [==============================] - 17s 343ms/step - loss: 5.2678\n",
            "Epoch 4/10\n",
            "50/50 [==============================] - 17s 335ms/step - loss: 5.2831\n",
            "Epoch 5/10\n",
            "50/50 [==============================] - 17s 343ms/step - loss: 4.9207\n",
            "Epoch 6/10\n",
            "50/50 [==============================] - 17s 333ms/step - loss: 4.9533\n",
            "Epoch 7/10\n",
            "50/50 [==============================] - 17s 345ms/step - loss: 4.6316\n",
            "Epoch 8/10\n",
            "50/50 [==============================] - 17s 337ms/step - loss: 4.6167\n",
            "Epoch 9/10\n",
            "50/50 [==============================] - 17s 342ms/step - loss: 4.3117\n",
            "Epoch 10/10\n",
            "50/50 [==============================] - 17s 342ms/step - loss: 4.3346\n",
            "Actual:    ['startseq baby holds game control above its head and laughs endseq', 'startseq baby is holding controller to video game above his head endseq', 'startseq little kid holds up remote control in front of the television endseq', 'startseq young boy is holding playstation controller over his head endseq', 'startseq baby sitting on floor and holding game controller over its head endseq']\n",
            "Predicted: startseq is is is is is is endseq\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Actual:    ['startseq man in an orange construction uniform wears white hardhat and sprays water hose endseq', 'startseq man in an orange safety suit spraying water endseq', 'startseq man in public road workers uniform sprays water endseq', 'startseq man wearing an orange uniform is working outside with hose endseq', 'startseq the man in the work uniform is using water hose endseq']\n",
            "Predicted: startseq man man with with window endseq\n",
            "train_score: 0.04988715715577709\n",
            "test_score: 0.05803093609932879\n",
            "          train      test\n",
            "count  1.000000  1.000000\n",
            "mean   0.049887  0.058031\n",
            "std         NaN       NaN\n",
            "min    0.049887  0.058031\n",
            "25%    0.049887  0.058031\n",
            "50%    0.049887  0.058031\n",
            "75%    0.049887  0.058031\n",
            "max    0.049887  0.058031\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vMF1RT4XIWu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define the captioning model\n",
        "def size_sm_fixed_vec(vocab_size, max_length):\n",
        "\t# feature extractor (encoder)\n",
        "\tinputs1 = Input(shape=(7, 7, 512))\n",
        "\tfe1 = GlobalMaxPooling2D()(inputs1)\n",
        "\tfe2 = Dense(64, activation='relu')(fe1)\n",
        "\tfe3 = RepeatVector(max_length)(fe2)\n",
        "\t# embedding\n",
        "\tinputs2 = Input(shape=(max_length,))\n",
        "\temb2 = Embedding(vocab_size, 50, mask_zero=True)(inputs2)\n",
        "\temb3 = LSTM(256, return_sequences=True)(emb2)\n",
        "\temb4 = TimeDistributed(Dense(64, activation='relu'))(emb3)\n",
        "\t# merge inputs\n",
        "\tmerged = concatenate([fe3, emb4])\n",
        "\t# language model (decoder)\n",
        "\tlm2 = LSTM(500)(merged)\n",
        "\tlm3 = Dense(500, activation='relu')(lm2)\n",
        "  \n",
        "\toutputs = Dense(vocab_size, activation='softmax')(lm3)\n",
        "\t# tie it together [image, seq] [word]\n",
        "\tmodel = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
        "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\treturn model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvZaHwWJYwrT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "e3b6fa5b-dcb7-46cc-ec37-2352a1fc4fab"
      },
      "source": [
        "# Generate Baseline model\n",
        "model_name = 'size_sm_fixed_vec'\n",
        "verbose = 1\n",
        "n_epochs = 10\n",
        "n_photos_per_update = 2\n",
        "n_batches_per_epoch = int(len(train) / n_photos_per_update)\n",
        "n_repeats = 1\n",
        "run_experiment(model_name, size_sm_fixed_vec, verbose, n_epochs, n_photos_per_update, n_batches_per_epoch, n_repeats)"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-93-1e07b1df91a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mn_batches_per_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mn_photos_per_update\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mn_repeats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mrun_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_sm_fixed_vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_photos_per_update\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_batches_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_repeats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-81-bb21a4a42371>\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m(model_name, func_name, verbose, n_epochs, n_photos_per_update, n_batches_per_epoch, n_repeats)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# fit model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m#data_generator(train_descriptions, train_features, tokenizer, max_length, n_photos_per_update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_descriptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_batches_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;31m# evaluate model on training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mtrain_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_model_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_descriptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1656\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1657\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1658\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1659\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    213\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    214\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1441\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1442\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m             class_weight=class_weight)\n\u001b[0m\u001b[1;32m   1444\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_uses_dynamic_learning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    755\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    758\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    129\u001b[0m                         \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    132\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected input_29 to have 4 dimensions, but got array with shape (67, 4096)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-hwA1EuoWoc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "outputId": "c78aec35-48a2-44c7-8fb0-e6843d3c7918"
      },
      "source": [
        "from pandas import read_csv\n",
        "from matplotlib import pyplot\n",
        " \n",
        "# load all .csv results into a dataframe\n",
        "train, test = DataFrame(), DataFrame()\n",
        "#directory = 'results'\n",
        "directory =  folderName+'/Result'\n",
        "for name in listdir(directory):\n",
        "\tif not name.endswith('csv'):\n",
        "\t\tcontinue\n",
        "\tfilename = directory + '/' + name\n",
        "\tdata = read_csv(filename, header=0)\n",
        "\texperiment = name.split('.')[0]\n",
        "\ttrain[experiment] = data['train']\n",
        "\ttest[experiment] = data['test']\n",
        " \n",
        "# plot results on train\n",
        "train.boxplot(vert=False)\n",
        "pyplot.show()\n",
        "# plot results on test\n",
        "test.boxplot(vert=False)\n",
        "pyplot.show()"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcMAAAD8CAYAAAAL3c8SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcMAAAD8CAYAAAAL3c8SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}